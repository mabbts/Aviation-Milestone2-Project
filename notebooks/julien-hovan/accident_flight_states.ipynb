{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import glob \n",
    "import numpy as np\n",
    "\n",
    "# ML - scikit-learn and pytorch libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the flight state vector paths (using the accident for now as querying a sample of normal flights atm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/raw/accident_flight_states/state_vectors_a52acc_1654088894_1654111587.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a255d9_1588773983_1588801421.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7e3cb_1580225813_1580235289.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a88cbf_1616951147_1616955197.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a052d4_1727960352_1727965975.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a81344_1657884224_1657890629.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6bac5_1697919371_1697934368.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_4ca9cb_1693357437_1693378258.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a0907b_1591456482_1591457426.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a18899_1675518416_1675524470.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a449a1_1696089576_1696094594.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad705e_1652276930_1652283412.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3ff82_1722000273_1722010516.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a144bb_1657195867_1657230380.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a2075c_1606236547_1606240815.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa7b24_1665837111_1665843579.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab03a3_1667775165_1667779076.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a14042_1726597330_1726600720.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8de7d_1714140330_1714153257.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5e44c_1685632274_1685642014.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a529fb_1673458275_1673469347.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a96375_1704473928_1704493833.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a0f305_1670624613_1670632031.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4fc02_1723403175_1723405018.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_adc96f_1581052149_1581055619.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4fb31_1642707644_1642710558.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5b26d_1589030684_1589051425.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a95052_1690116433_1690121427.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_acb0ea_1689881063_1689889931.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a262bf_1697050375_1697070498.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5d536_1641905569_1641916270.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a31d71_1660394422_1660411717.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a60b1c_1626698766_1626701649.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a75161_1639154078_1639171831.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a62e2c_1670932515_1670945319.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9c819_1668860891_1668873432.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a18f2b_1737811267_1737826012.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5d2e6_1600256626_1600278596.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5680a_1612719257_1612722126.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_abf049_1680202733_1680215811.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5aba8_1722169421_1722176441.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5bbd3_1712247388_1712248795.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_abe056_1633722134_1633731398.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab8e3a_1607700409_1607701585.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a004bb_1584201158_1584202191.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a576e7_1713268667_1713276593.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab2504_1643933825_1643943290.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa6914_1682149904_1682182402.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad3ebe_1631373073_1631378105.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a047e0_1693145913_1693164523.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5fb78_1709054612_1709058618.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a38706_1659186577_1659206739.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6a1ab_1682690855_1682695692.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4adfb_1604678698_1604681069.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a448f5_1655793940_1655806869.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a69f08_1668278438_1668281542.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4fe61_1729094739_1729099078.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1e74d_1594424425_1594440038.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a339e0_1639356036_1639365069.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8b35a_1702913469_1702932173.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a13e73_1721649896_1721663608.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa74c4_1704033492_1704061483.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6b0a0_1594661223_1594665875.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5bb14_1656677835_1656681979.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a658ee_1731691643_1731706460.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aac337_1722690445_1722729927.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9bd1d_1724345898_1724355603.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a88bed_1726247759_1726258161.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9ee99_1580482061_1580483653.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad9f42_1699716829_1699721049.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ac2b61_1703084950_1703091209.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7eff7_1581040142_1581041315.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8886d_1670489768_1670521254.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9db1c_1681494134_1681495822.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9fdda_1621513062_1621518910.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6740e_1685550568_1685552605.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8487d_1609244872_1609245839.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_accc82_1712935746_1712950274.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ac4fbd_1636139130_1636147470.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aca56c_1684678689_1684683345.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6ed4a_1691761591_1691770799.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab25be_1646604550_1646621075.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1f94c_1723039225_1723047958.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_8963eb_1580603553_1580657489.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa2fb0_1658361693_1658373874.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a63e94_1589914697_1589920858.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_c06848_1686061755_1686064550.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a35ca2_1718111257_1718116082.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8f3c5_1691029788_1691042803.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9675d_1638183224_1638201427.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4d9d8_1709994817_1710001074.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a255e9_1666540455_1666566308.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a16adc_1578693835_1578702016.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3e2b2_1643719870_1643726415.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a008b9_1667501538_1667511119.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a54d3c_1682169830_1682176701.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7e2e0_1598662371_1598678091.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_adedba_1611831523_1611834435.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a07fa2_1677943116_1677948899.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aab001_1707583404_1707599148.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a54836_1713536556_1713540856.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4400e_1579900407_1579913268.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1597d_1712426043_1712440774.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4016a_1610466824_1610476547.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a6ffcb_1679921484_1679925343.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a06073_1687626828_1687631029.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a86655_1728398677_1728402177.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7f457_1685890744_1685896174.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1fef3_1713956107_1713957323.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7c31c_1726320985_1726326522.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a41388_1650133826_1650139704.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_abcf23_1601490300_1601507775.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3daa1_1704377243_1704380339.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ac0e49_1633289787_1633294251.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a648d2_1736877161_1736883984.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a49f82_1692563198_1692564075.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a24302_1712332297_1712333471.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a11731_1700675776_1700681299.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a68414_1667421723_1667433354.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4822b_1692626754_1692634540.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a88774_1691043824_1691058463.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a95962_1732637297_1732648359.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a564b3_1664996853_1665000978.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a67a4d_1589989577_1590000899.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a982ab_1619099981_1619101486.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_adc0cf_1692276542_1692283298.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a252ae_1689174139_1689180557.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4e38a_1722172991_1722193349.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a35501_1662219799_1662222411.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab01f9_1584815593_1584822794.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab6e51_1647532564_1647539697.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a52390_1623603493_1623604915.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a04b45_1699373104_1699378335.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aba05d_1718295572_1718297264.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab1fb8_1704610714_1704619043.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a195b4_1714671156_1714673999.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a18eb3_1616426340_1616440701.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a02efc_1710253851_1710262077.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7b12a_1711807935_1711810123.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a457a4_1600627422_1600639850.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3dc89_1612713381_1612714732.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a4fb34_1662726065_1662729504.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a718b3_1732464987_1732472845.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad2f18_1654657004_1654664095.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3ad27_1728857855_1728870126.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ab273d_1733932578_1733950303.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a47546_1708875227_1708893699.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1ee22_1686240587_1686252365.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa2190_1727879719_1727884671.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a16adc_1679505180_1679511505.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a328b8_1625574676_1625582987.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ac5b49_1598978313_1598999562.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a14a39_1646764736_1646779700.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3310b_1722611892_1722622889.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a74923_1695599188_1695633404.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aba513_1680373552_1680382750.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a203b0_1690078670_1690086589.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad1574_1714760096_1714769397.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a8d5dc_1658493449_1658510937.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_abb5a3_1670089141_1670091917.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aab202_1615816324_1615823986.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aaf0c6_1695889929_1695914924.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ac05c1_1726143882_1726160171.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a06538_1729079429_1729082276.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a46d0d_1671117800_1671130284.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a42323_1677769222_1677774774.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a5bca5_1648577715_1648583453.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aaf83c_1629550735_1629559573.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_c05bc2_1665855632_1665865829.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a00314_1625150614_1625172175.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9f1ea_1693746879_1693759919.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9ac7d_1704125498_1704127769.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a417af_1611945951_1611949925.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a0dba6_1690471658_1690476607.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a2685b_1593960196_1593967497.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a26fc9_1653162605_1653170519.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a9ee41_1708955705_1708976061.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a010b3_1706462040_1706467389.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1ba1d_1646751459_1646774194.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aaba9a_1679391510_1679424774.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_abc466_1727958725_1727963897.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a7622c_1682800801_1682805160.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aa7718_1724077366_1724080509.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a67a82_1675273520_1675277188.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a1b30e_1624272233_1624293026.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_aaece5_1628999793_1629062252.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a2dc36_1719595531_1719604644.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_c00647_1718601280_1718602342.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad06e7_1650030977_1650033759.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_ad1c11_1676065930_1676074258.parquet',\n",
       " '../../data/raw/accident_flight_states/state_vectors_a3881c_1716067032_1716073176.parquet']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flight_state_vectors_paths = glob.glob(\"../../data/raw/accident_flight_states/*\")\n",
    "\n",
    "flight_state_vectors_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 392310 entries, 0 to 392309\n",
      "Data columns (total 13 columns):\n",
      " #   Column        Non-Null Count   Dtype          \n",
      "---  ------        --------------   -----          \n",
      " 0   time          392310 non-null  datetime64[ns] \n",
      " 1   icao24        258056 non-null  string         \n",
      " 2   lat           392305 non-null  double[pyarrow]\n",
      " 3   lon           392305 non-null  double[pyarrow]\n",
      " 4   velocity      392115 non-null  double[pyarrow]\n",
      " 5   heading       392115 non-null  double[pyarrow]\n",
      " 6   vertrate      392115 non-null  double[pyarrow]\n",
      " 7   callsign      253998 non-null  string         \n",
      " 8   onground      258056 non-null  bool[pyarrow]  \n",
      " 9   spi           258056 non-null  bool[pyarrow]  \n",
      " 10  squawk        118846 non-null  string         \n",
      " 11  geoaltitude   392095 non-null  double[pyarrow]\n",
      " 12  baroaltitude  392303 non-null  double[pyarrow]\n",
      "dtypes: bool[pyarrow](2), datetime64[ns](1), double[pyarrow](7), string(3)\n",
      "memory usage: 37.1 MB\n"
     ]
    }
   ],
   "source": [
    "# Process flight state vector data by:\n",
    "# 1. Loading individual parquet files containing state vectors\n",
    "# 2. Resampling each flight's data to 5 second intervals to standardize the sampling rate\n",
    "# 3. Combining all resampled flight data into a single DataFrame\n",
    "\n",
    "def resample_flight_state_data(df, interval='5s'):\n",
    "    \"\"\"\n",
    "    Resample flight state vector data to a specified time interval.\n",
    "    \n",
    "    This function takes raw flight state data with irregular time intervals and resamples it\n",
    "    to create evenly spaced observations. For each new time interval, it takes the first \n",
    "    available state vector if multiple observations exist within that interval.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): DataFrame containing flight state vector data. Must have a 'time' \n",
    "                          column with Unix timestamps.\n",
    "        interval (str): Pandas time interval string specifying the desired sampling rate.\n",
    "                       Default is '5s' for 5 seconds. See pandas documentation for other options.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with state vectors resampled to the specified interval.\n",
    "                     The time column is converted to datetime and all rows are sorted chronologically.\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying the original data\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Convert Unix timestamps to pandas datetime objects\n",
    "    df_copy['time'] = pd.to_datetime(df_copy['time'], unit='s')\n",
    "    \n",
    "    # Ensure data is sorted chronologically\n",
    "    df_copy = df_copy.sort_values('time')\n",
    "    \n",
    "    # Set time as index for resampling\n",
    "    df_copy.set_index('time', inplace=True)\n",
    "    \n",
    "    # Resample data:\n",
    "    # - Creates new rows at regular intervals specified by 'interval'\n",
    "    # - Takes the first observation in each interval\n",
    "    # - Resets index to make time a regular column again\n",
    "    resampled_df = df_copy.resample(interval).first().reset_index()\n",
    "    \n",
    "    return resampled_df\n",
    "\n",
    "# Initialize list to store resampled DataFrames\n",
    "resampled_dfs = []\n",
    "\n",
    "# Process each flight's state vector file with forward fill imputation\n",
    "for path in flight_state_vectors_paths:\n",
    "    # Load the raw state vector data\n",
    "    df = pd.read_parquet(path)\n",
    "    \n",
    "    # Resample to 5-second intervals\n",
    "    resampled_df = resample_flight_state_data(df)\n",
    "    \n",
    "    # Drop columns that are entirely NaN\n",
    "    resampled_df = resampled_df.dropna(axis=1, how='all')\n",
    "\n",
    "    resampled_df = resampled_df.sort_values('time')  # ensure order\n",
    "    resampled_df.set_index('time', inplace=True)  # if you have a time index\n",
    "    \n",
    "    # interpolate missing values\n",
    "    resampled_df[['lat', 'lon', 'velocity', 'heading', 'vertrate', 'geoaltitude', 'baroaltitude']] = resampled_df[['lat', 'lon', 'velocity', 'heading', 'vertrate', 'geoaltitude', 'baroaltitude']].interpolate(method='time')\n",
    "    \n",
    "    # unset index\n",
    "    resampled_df.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    # Append only non-empty DataFrames to the list\n",
    "    if not resampled_df.empty:\n",
    "        resampled_dfs.append(resampled_df)\n",
    "\n",
    "# Concatenate all imputed and resampled DataFrames\n",
    "resampled_df = pd.concat(resampled_dfs, ignore_index=True)\n",
    "\n",
    "# Display information about the final DataFrame\n",
    "resampled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                 0\n",
       "icao24          134254\n",
       "lat                  5\n",
       "lon                  5\n",
       "velocity           195\n",
       "heading            195\n",
       "vertrate           195\n",
       "callsign        138312\n",
       "onground        134254\n",
       "spi             134254\n",
       "squawk          273464\n",
       "geoaltitude        215\n",
       "baroaltitude         7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets check missing values\n",
    "resampled_df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time                 0\n",
       "icao24          134251\n",
       "lat                  0\n",
       "lon                  0\n",
       "velocity             0\n",
       "heading              0\n",
       "vertrate             0\n",
       "callsign        138297\n",
       "onground        134251\n",
       "spi             134251\n",
       "squawk          273436\n",
       "geoaltitude          0\n",
       "baroaltitude         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets drop rows with missing - lat, lon, velocity, heading, vertrate, geoaltitude, baroaltitude\n",
    "resampled_df = resampled_df.dropna(subset=['lat', 'lon', 'velocity', 'heading', 'vertrate', 'geoaltitude', 'baroaltitude'])\n",
    "\n",
    "# lets check missing values again\n",
    "resampled_df.isna().sum()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the input and output sequences, creating this sliding window of 17 steps and predicting the next 1 step\n",
    "\n",
    "we first define `create_input_output_sequences` function which will \n",
    "- Each input window (X) contains 17 consecutive time steps of flight data\n",
    "- Each output (y) contains the next 1 time step after the input window\n",
    "- The function slides this window through the entire flight, creating many (X,y) pairs for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_output_sequences(\n",
    "    df_flight, \n",
    "    input_len=17, \n",
    "    pred_len=1, \n",
    "    feature_cols=None, \n",
    "    target_cols=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create pairs of (X, y) from a single flight's data for one-step-ahead prediction:\n",
    "      - X: the first 'input_len' time steps\n",
    "      - y: the next 'pred_len' time steps (1 in this case).\n",
    "    \n",
    "    Args:\n",
    "        df_flight (pd.DataFrame): Time-sorted data for a single flight.\n",
    "        input_len (int): Number of historical time steps used as input.\n",
    "        pred_len (int): Number of time steps to predict. For single-step, it's 1.\n",
    "        feature_cols (list): Columns to use as inputs (X).\n",
    "        target_cols (list): Columns to predict (y).\n",
    "        \n",
    "    Returns:\n",
    "        X_array (np.ndarray): shape (num_samples, input_len, num_features)\n",
    "        y_array (np.ndarray): shape (num_samples, pred_len, num_targets)\n",
    "          - For single-step forecast, pred_len=1, so shape = (num_samples, 1, num_targets).\n",
    "    \"\"\"\n",
    "    # If not specified, assume all numeric columns except flight_id/time are features\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [c for c in df_flight.columns \n",
    "                        if c not in ['flight_id', 'time']]\n",
    "    if target_cols is None:\n",
    "        # Predict same as feature columns\n",
    "        target_cols = feature_cols\n",
    "    \n",
    "    # Convert to numpy\n",
    "    feature_values = df_flight[feature_cols].values\n",
    "    target_values = df_flight[target_cols].values\n",
    "    \n",
    "    # We'll collect sequences here\n",
    "    X_list, y_list = [], []\n",
    "    \n",
    "    n = len(df_flight)\n",
    "    # We need a total of input_len + pred_len steps for each sample\n",
    "    # e.g. 17 + 1 = 18\n",
    "    seq_len = input_len + pred_len\n",
    "    \n",
    "    for start_idx in range(n - seq_len + 1):\n",
    "        # X = [start_idx : start_idx+17]\n",
    "        X_seq = feature_values[start_idx : start_idx + input_len]\n",
    "        # y = [start_idx+17 : start_idx+17+1]\n",
    "        y_seq = target_values[start_idx + input_len : start_idx + input_len + pred_len]\n",
    "        \n",
    "        X_list.append(X_seq)\n",
    "        y_list.append(y_seq)\n",
    "        \n",
    "    X_array = np.array(X_list)  # shape: (num_samples, 17, num_features)\n",
    "    y_array = np.array(y_list)  # shape: (num_samples, 1, num_targets)\n",
    "    \n",
    "    return X_array, y_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_final shape: (254614, 17, 7)\n",
      "y_final shape: (254614, 1, 7)\n"
     ]
    }
   ],
   "source": [
    "# the lists to store the input and output sequences\n",
    "all_X = []\n",
    "all_y = []\n",
    "\n",
    "for flight_id, df_group in resampled_df.groupby('icao24'):\n",
    "    # Sort by time\n",
    "    df_group = df_group.sort_values('time').reset_index(drop=True)\n",
    "    \n",
    "    # Example feature columns\n",
    "    feature_cols = ['lon','lat','heading','velocity','vertrate', 'heading', 'geoaltitude']\n",
    "    # Example target columns: let's assume we want to predict the same set \n",
    "    target_cols = ['lon','lat','heading','velocity','vertrate', 'heading', 'geoaltitude']\n",
    "    \n",
    "    X_flight, y_flight = create_input_output_sequences(\n",
    "        df_group, \n",
    "        input_len=17, \n",
    "        pred_len=1, \n",
    "        feature_cols=feature_cols, \n",
    "        target_cols=target_cols\n",
    "    )\n",
    "    \n",
    "    all_X.append(X_flight)\n",
    "    all_y.append(y_flight)\n",
    "\n",
    "# Concatenate across all flights\n",
    "X_final = np.concatenate(all_X, axis=0)  # shape: (total_samples, 17, num_features)\n",
    "y_final = np.concatenate(all_y, axis=0)  # shape: (total_samples, 1, num_targets)\n",
    "\n",
    "print(\"X_final shape:\", X_final.shape)\n",
    "print(\"y_final shape:\", y_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_final, y_final, test_size=0.2, shuffle=True, random_state=42 # use a random state to ensure reproducibility\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (203691, 17, 7)\n",
      "X_train dtype: object\n",
      "y_train shape: (203691, 1, 7)\n",
      "y_train dtype: object\n",
      "X_test shape: (50923, 17, 7)\n",
      "X_test dtype: object\n",
      "y_test shape: (50923, 1, 7)\n",
      "y_test dtype: object\n"
     ]
    }
   ],
   "source": [
    "# show examples of the data and its shape\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_train dtype:\", X_train.dtype)\n",
    "print(\"y_train shape:\", y_train.shape) \n",
    "print(\"y_train dtype:\", y_train.dtype)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"X_test dtype:\", X_test.dtype)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"y_test dtype:\", y_test.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "Now we can get into training the model, first we define the model using torch neural network modules\n",
    "\n",
    "**Model Overview**  \n",
    "\n",
    "- **Input Projection**: A linear layer projects the input features to a higher-dimensional space (`d_model=64`).\n",
    "- **Positional Encoding**: Since Transformers do not inherently understand sequence order, we add positional encodings.\n",
    "- **Transformer Encoder**: A multi-layer Transformer encoder processes the sequence using self-attention and feedforward layers.\n",
    "- **Final Prediction**: We extract the final timestepâ€™s representation from the Transformer output and pass it through a fully connected layer to predict the next state.\n",
    "\n",
    "**Code Structure**  \n",
    "\n",
    "- `TransformerPredictor`: Defines the Transformer-based model.\n",
    "- `PositionalEncoding`: Implements sinusoidal positional encoding to provide sequence information.\n",
    "\n",
    "The following cell contains the full implementation of the model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=5, d_model=64, nhead=4, num_layers=2, \n",
    "                 dim_feedforward=128, dropout=0.1, target_dim=5):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "        \n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True  # If using PyTorch 1.10+ for batch_first in the encoder\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # We'll predict next state => shape (batch_size, target_dim)\n",
    "        self.fc_out = nn.Linear(d_model, target_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len=17, input_dim=5)\n",
    "        Returns: (batch_size, target_dim=5)\n",
    "        \"\"\"\n",
    "        # Project input to d_model\n",
    "        x = self.input_proj(x)  # shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Add positional encoding\n",
    "        x = self.pos_encoder(x)  # shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # Pass through Transformer encoder\n",
    "        encoded = self.transformer_encoder(x)  # shape: (batch_size, seq_len, d_model)\n",
    "        \n",
    "        # We only want the final time-step's representation (seq_len-1 = 16)\n",
    "        last_step = encoded[:, -1, :]  # shape: (batch_size, d_model)\n",
    "        \n",
    "        # Predict next state\n",
    "        out = self.fc_out(last_step)  # (batch_size, target_dim)\n",
    "        return out\n",
    "\n",
    "# Example positional encoding implementation\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # shape: (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        x = x + self.pe[:, :seq_len, :]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA available: True\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3070\n"
     ]
    }
   ],
   "source": [
    "# ensuring we are using the GPU\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "    print(f\"Device name: {torch.cuda.get_device_name()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 3316818.0741\n",
      "Epoch 1, Loss: 178314.2192\n",
      "Epoch 2, Loss: 43906.9187\n",
      "Epoch 3, Loss: 41371.2203\n",
      "Epoch 4, Loss: 36641.0136\n",
      "Epoch 5, Loss: 34304.3319\n",
      "Epoch 6, Loss: 31459.4843\n",
      "Epoch 7, Loss: 32426.3013\n",
      "Epoch 8, Loss: 34311.9144\n",
      "Epoch 9, Loss: 32364.1478\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to proper numeric type before torch conversion\n",
    "X_train = X_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float().to(device)  # (num_samples, 17, 7)\n",
    "y_train_t = torch.from_numpy(y_train).float().to(device)  # (num_samples, 1, 7)\n",
    "y_train_t = y_train_t.squeeze(1)               # shape: (num_samples, 7), if single-step\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = TransformerPredictor(input_dim=7, target_dim=7)\n",
    "criterion = nn.MSELoss()  # or L1Loss, depends on your preference\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_batch)  # shape (batch_size, 5)\n",
    "        loss = criterion(output, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drop in loss is promising ater only 10 epochs -> transformers need a lot of epochs to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 21849.83203125\n"
     ]
    }
   ],
   "source": [
    "X_test = X_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "model.eval()\n",
    "X_test_t = torch.from_numpy(X_test).float().to(device)\n",
    "y_test_t = torch.from_numpy(y_test).float().squeeze(1).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = model(X_test_t)\n",
    "    test_loss = criterion(preds, y_test_t)\n",
    "    \n",
    "print(\"Test Loss:\", test_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_state(state, columns):\n",
    "    \"\"\"\n",
    "    Print the state in a human-readable format.\n",
    "\n",
    "    Args:\n",
    "        state (np.ndarray): 1D array of flight state values.\n",
    "        columns (list of str): Names of the flight state dimensions.\n",
    "    \"\"\"\n",
    "    for name, value in zip(columns, state):\n",
    "        print(f\"{name:15s}: {value:.2f}\")\n",
    "\n",
    "def print_prediction(input_seq, prediction, input_columns=None, pred_columns=None):\n",
    "    \"\"\"\n",
    "    Print the last timestep of the input sequence and the predicted next state.\n",
    "\n",
    "    Args:\n",
    "        input_seq (np.ndarray): 2D array of shape (seq_len, num_features) for the input sequence.\n",
    "        prediction (np.ndarray): 1D array of predicted flight state values.\n",
    "        input_columns (list, optional): Column names for the input state.\n",
    "        pred_columns (list, optional): Column names for the predicted state.\n",
    "    \"\"\"\n",
    "    print(\"==== Flight State Prediction ====\\n\")\n",
    "    print(\"Input Sequence (Last Timestep):\")\n",
    "    if input_columns is not None:\n",
    "        last_input = input_seq[-1]\n",
    "        for name, value in zip(input_columns, last_input):\n",
    "            print(f\"  {name:15s}: {value:.2f}\")\n",
    "    else:\n",
    "        print(input_seq[-1])\n",
    "    \n",
    "    print(\"\\nPredicted Next State:\")\n",
    "    if pred_columns is not None:\n",
    "        pretty_print_state(prediction, pred_columns)\n",
    "    else:\n",
    "        print(prediction)\n",
    "\n",
    "state_columns = [\"lon\", \"lat\", \"heading1\", \"velocity\", \"vertrate\", \"heading2\", \"geoaltitude\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Flight State Prediction ====\n",
      "\n",
      "Input Sequence (Last Timestep):\n",
      "  lon            : -2.61\n",
      "  lat            : 53.54\n",
      "  heading1       : 270.26\n",
      "  velocity       : 225.84\n",
      "  vertrate       : 0.65\n",
      "  heading2       : 270.26\n",
      "  geoaltitude    : 10919.46\n",
      "\n",
      "Predicted Next State:\n",
      "lon            : -36.61\n",
      "lat            : 42.05\n",
      "heading1       : 286.57\n",
      "velocity       : 244.74\n",
      "vertrate       : -0.47\n",
      "heading2       : 286.66\n",
      "geoaltitude    : 10669.73\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Generate a Prediction ----\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # Get a sample input sequence (take the first sample from the test set)\n",
    "    sample_input = X_test_t[0].unsqueeze(0)  # shape: (1, 17, 7)\n",
    "    prediction = model(sample_input)         # shape: (1, 7)\n",
    "\n",
    "# Remove the batch dimension and convert predictions (and input) to NumPy arrays.\n",
    "prediction = prediction.squeeze(0).cpu().numpy()\n",
    "sample_input_np = X_test_t[0].cpu().numpy()\n",
    "\n",
    "# Print out the last timestep from the input and the predicted next state.\n",
    "print_prediction(sample_input_np, prediction, input_columns=state_columns, pred_columns=state_columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
